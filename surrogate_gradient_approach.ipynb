{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our data loader. Should get an index file for an experment type (9mm,5mm etc). The index file should contain a path to each indiviual training experiment and the class it belongs to high, medium, or low.\n",
    "Note that the class is represented as a int, the map is shown below\n",
    "\n",
    "|class         | numerical value|\n",
    "|--------------|----------------|\n",
    "|<i>high</i>   |               2|\n",
    "|<i>medium</i> |               1|\n",
    "|<i>low</i>    |               0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example index file (Prepocessing/data/spikeTrains/1.5-SpikeTrains/index.csv)\n",
    "<pre>\n",
    "spikeTrain_1.csv,0\n",
    "spikeTrain_2.csv,0\n",
    "spikeTrain_3.csv,1\n",
    "spikeTrain_4.csv,0\n",
    "spikeTrain_5.csv,2\n",
    "spikeTrain_6.csv,1\n",
    "</pre>\n",
    "\n",
    "\n",
    "Example spik train file (Prepocessing/data/spikeTrains/1.5-SpikeTrains/spikeTrain_1.csv)\n",
    "<pre>\n",
    "0\n",
    "0\n",
    "0\n",
    ".\n",
    ".\n",
    ".\n",
    "0\n",
    "0\n",
    "1\n",
    "0\n",
    ".\n",
    ".\n",
    ".\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation\n",
    "\n",
    "class expermentDataloader(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_file: str, \n",
    "        data_path: str,\n",
    "    ):\n",
    "        super(expermentDataloader, self).__init__()\n",
    "        self.root_dir = data_path\n",
    "        self.expermentSikeTrainsIndex = pd.read_csv(index_file) # self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.inputs = [\n",
    "            f\"{os.path.join(self.expermentSikeTrainsIndex.iloc[i, 0])}\" for i in range(len(self.expermentSikeTrainsIndex)) \n",
    "        ]\n",
    "        self.targets = [\n",
    "            f\"{os.path.join(self.expermentSikeTrainsIndex.iloc[i, 1])}\" for i in range(len(self.expermentSikeTrainsIndex)) \n",
    "        ]\n",
    "\n",
    "    def _fileToSlayerEvents(self, fileName: str):\n",
    "        CSVlines = pd.read_csv(os.path.join(self.root_dir,fileName)).to_numpy()\n",
    "        events = np.array(torch.FloatTensor(CSVlines))\n",
    "        \n",
    "        x_event = np.flip(events[:, 0])\n",
    "        y_event = None\n",
    "        c_event = torch.zeros(len(x_event), )\n",
    "        t_event = np.flip(events[:, 1])\n",
    "        return slayer.io.Event(x_event,y_event,c_event,t_event)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = self._fileToSlayerEvents(self.inputs[index])\n",
    "        target = self._fileToSlayerEvents(self.targets[index])\n",
    "        \n",
    "        return (\n",
    "            input.fill_tensor(torch.zeros(1, 1, 200, 25000)).squeeze(), # input spike train\n",
    "            target.fill_tensor(torch.zeros(1, 1, 200, 25000)).squeeze() # target spike train\n",
    "        )\n",
    "        # return torch.FloatTensor(CSVlines.flatten()), int(eventClass)\n",
    "\n",
    "    def getSlayerEvents(self, index: int):\n",
    "        input = self._fileToSlayerEvents(self.inputs[index])\n",
    "        target = self._fileToSlayerEvents(self.targets[index])\n",
    "        \n",
    "        return (\n",
    "            input, # input spike train\n",
    "            target # target spike train\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.expermentSikeTrainsIndex)\n",
    "    \n",
    "indexFile5mm = \"./Prepocessing/data/test/output/index.csv\"\n",
    "PathTo5mmSpikeTrains = \"./Prepocessing/data/test/output\"\n",
    "\n",
    "trainingData = expermentDataloader(indexFile5mm,PathTo5mmSpikeTrains)\n",
    "\n",
    "print(len(trainingData[0][0][0]))\n",
    "print(len(trainingData[0][1][0]))\n",
    "trainingData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_anim  = trainingData.getSlayerEvents(0)[0].anim(plt.figure(figsize=(10, 10)))\n",
    "target_anim = trainingData.getSlayerEvents(0)[1].anim(plt.figure(figsize=(10, 10)))\n",
    "\n",
    "## This produces interactive animation\n",
    "# display.HTML(input_anim.to_jshtml())\n",
    "# display.HTML(target_anim.to_jshtml())\n",
    "\n",
    "## Saving and loading gif for better animation in github\n",
    "input_anim.save('input.gif', animation.PillowWriter(fps=24), dpi=300)\n",
    "target_anim.save('target.gif', animation.PillowWriter(fps=24), dpi=300)\n",
    "\n",
    "gif_td = lambda gif: f'<td> <img src=\"{gif}\" alt=\"Drawing\" style=\"height: 400px;\"/> </td>'\n",
    "html = '<table><tr>'\n",
    "html += '<td> Input </td><td> Target </td></tr><tr>'\n",
    "html += gif_td(f'input.gif')\n",
    "html += gif_td(f'target.gif')\n",
    "html += '</tr></table>'\n",
    "display.HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"./Prepocessing/data/test/output/index.csv\"\n",
    "dataFolder = \"./Prepocessing/data/test/output\"\n",
    "\n",
    "trainingData = expermentDataloader(index,dataFolder)\n",
    "\n",
    "print(f\"Is NOT all zeros: {np.any(np.array(trainingData[0][0]))}\")\n",
    "print(len(trainingData[0][0]))\n",
    "print(f\"Number of spikes: {sum(trainingData[0][0])}\")\n",
    "trainingData[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Is NOT all zeros: {np.any(np.array(trainingData[1][0]))}\")\n",
    "print(len(trainingData[1][0]))\n",
    "print(f\"Number of spikes: {sum(trainingData[1][0])}\")\n",
    "trainingData[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Is NOT all zeros: {np.any(np.array(trainingData[len(trainingData)-1][0]))}\")\n",
    "print(len(trainingData[len(trainingData)-1][0]))\n",
    "print(f\"Number of spikes: {sum(trainingData[len(trainingData)-1][0])}\")\n",
    "trainingData[len(trainingData)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_spike_train = len(trainingData[len(trainingData)-1][0])\n",
    "steps_per_spike_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### setup dataloader for pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=trainingData, batch_size=3)\n",
    "i = iter(train_loader)\n",
    "example = next(i)\n",
    "type(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example[0][0].any())\n",
    "print(example[1])\n",
    "example = next(i)\n",
    "print(example[0][0].any())\n",
    "print(example[1])\n",
    "example = next(i)\n",
    "print(example[0][0].any())\n",
    "print(example[1])\n",
    "example = next(i)\n",
    "print(example[0][0].any())\n",
    "print(example[1])\n",
    "example[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shoul see the classes 1,0,2 and all true for any (i.e. they all have at least one event)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Building network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `slayer.block` is a combination of `synapse`, `dendrite`, `neuron` and `axon` components. This allows for easier devlepment of SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        neuron_params = {\n",
    "                'threshold'     : 0.1,\n",
    "                'current_decay' : 1,\n",
    "                'voltage_decay' : 0.1,\n",
    "                'requires_grad' : True,     \n",
    "            }\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                slayer.block.cuba.Dense(neuron_params, 200, 256),\n",
    "                slayer.block.cuba.Dense(neuron_params, 256, 200),\n",
    "            ])\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        return spike\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_folder = 'Trained'\n",
    "os.makedirs(trained_folder, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "net = Network().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pick between the following \n",
    "* `SpikeTime`: precise spike time based loss when target spike train is known.\n",
    "* `SpikeRate`: spike rate based loss when desired rate of the output neuron is known.\n",
    "* `SpikeMax`: negative log likelihood losses for classification without any rate tuning.\n",
    "Sense we dont know the output spike train (as of now) and we dont have a desired spike rate we use `SpikeMax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = slayer.loss.SpikeTime().to(device)\n",
    "stats = slayer.utils.LearningStats()\n",
    "assistant = slayer.utils.Assistant(net, error, optimizer, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (input, target) in enumerate(train_loader): # training loop\n",
    "        output = assistant.train(input, target)\n",
    "        print(f'\\r[Epoch {epoch:3d}/{epochs}] {stats}', end='')\n",
    "    \n",
    "    if stats.training.best_loss:\n",
    "        torch.save(net.state_dict(), trained_folder + '/network.pt')\n",
    "    stats.update()\n",
    "    stats.save(trained_folder + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
