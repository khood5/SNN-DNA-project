{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our data loader. Should get an index file for an experment type (9mm,5mm etc). The index file should contain a path to each indiviual training experiment and the class it belongs to high, medium, or low.\n",
    "Note that the class is represented as a int, the map is shown below\n",
    "\n",
    "|class         | numerical value|\n",
    "|--------------|----------------|\n",
    "|<i>high</i>   |               2|\n",
    "|<i>medium</i> |               1|\n",
    "|<i>low</i>    |               0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example index file (Prepocessing/data/spikeTrains/1.5-SpikeTrains/index.csv)\n",
    "<pre>\n",
    "spikeTrain_1.csv,0\n",
    "spikeTrain_2.csv,0\n",
    "spikeTrain_3.csv,1\n",
    "spikeTrain_4.csv,0\n",
    "spikeTrain_5.csv,2\n",
    "spikeTrain_6.csv,1\n",
    "</pre>\n",
    "\n",
    "\n",
    "Example spik train file (Prepocessing/data/spikeTrains/1.5-SpikeTrains/spikeTrain_1.csv)\n",
    "<pre>\n",
    "0\n",
    "0\n",
    "0\n",
    ".\n",
    ".\n",
    ".\n",
    "0\n",
    "0\n",
    "1\n",
    "0\n",
    ".\n",
    ".\n",
    ".\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation\n",
    "\n",
    "class expermentDataloader(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_file: str, \n",
    "        data_path: str,\n",
    "    ):\n",
    "        super(expermentDataloader, self).__init__()\n",
    "        self.root_dir = data_path\n",
    "        self.expermentSikeTrainsIndex = pd.read_csv(index_file) # self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.inputs = [\n",
    "            f\"{os.path.join(self.expermentSikeTrainsIndex.iloc[i, 0])}\" for i in range(len(self.expermentSikeTrainsIndex)) \n",
    "        ]\n",
    "        self.targets = [\n",
    "            f\"{os.path.join(self.expermentSikeTrainsIndex.iloc[i, 1])}\" for i in range(len(self.expermentSikeTrainsIndex)) \n",
    "        ]\n",
    "\n",
    "    def _fileToSlayerEvents(self, fileName: str):\n",
    "        CSVlines = pd.read_csv(os.path.join(self.root_dir,fileName)).to_numpy()\n",
    "        events = np.array(torch.FloatTensor(CSVlines))\n",
    "        \n",
    "        x_event = events[:, 0]\n",
    "        y_event = None\n",
    "        c_event = torch.zeros(len(x_event), )\n",
    "        t_event = events[:, 1]\n",
    "        return slayer.io.Event(x_event,y_event,c_event,t_event)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = self._fileToSlayerEvents(self.inputs[index])\n",
    "        target = self._fileToSlayerEvents(self.targets[index])\n",
    "        \n",
    "        return (\n",
    "            input.fill_tensor(torch.zeros(1, 1, 1, 3000)).squeeze(), # input spike train\n",
    "            target.fill_tensor(torch.zeros(1, 1, 1, 3000)).squeeze() # target spike train\n",
    "        )\n",
    "        # return torch.FloatTensor(CSVlines.flatten()), int(eventClass)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.expermentSikeTrainsIndex)\n",
    "    \n",
    "indexFile5mm = \"./Prepocessing/data/spikeTrainsInputTarget/5-SpikeTrains/index.csv\"\n",
    "PathTo5mmSpikeTrains = \"./Prepocessing/data/spikeTrainsInputTarget/5-SpikeTrains\"\n",
    "\n",
    "trainingData = expermentDataloader(indexFile5mm,PathTo5mmSpikeTrains)\n",
    "trainingData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return Event(x_event, None, c_event, t_event / 1000)\n",
    "# indexFile5mm = \"./Prepocessing/data/spikeTrains/5-SpikeTrains/index.csv\"\n",
    "# PathTo5mmSpikeTrains = \"./Prepocessing/data/spikeTrains/5-SpikeTrains\"\n",
    "\n",
    "# trainingData = expermentDataloader(indexFile5mm,PathTo5mmSpikeTrains)\n",
    "# x_event = torch.zeros(len(trainingData[0][0].nonzero()[0]), )\n",
    "# y_event = None\n",
    "# c_event = torch.zeros(len(trainingData[0][0].nonzero()[0]), )\n",
    "# t_event = np.array(trainingData[0][0].nonzero()[0])\n",
    "\n",
    "# # x_event_2 = trainingData[0][0] * torch.zeros(len(trainingData[0][0]), )\n",
    "# # y_event_2 = None\n",
    "# # c_event_2 = torch.zeros(len(trainingData[0][0]), )\n",
    "# # t_event_2 = np.array(list(range(len(trainingData[0][0]))))\n",
    "# # print(c_event)\n",
    "# slayer.io.Event(x_event,y_event,c_event,t_event).to_tensor().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is NOT all zeros: True\n",
      "2\n",
      "Number of spikes: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexFile5mm = \"./Prepocessing/data/spikeTrains/5-SpikeTrains/index.csv\"\n",
    "PathTo5mmSpikeTrains = \"./Prepocessing/data/spikeTrains/5-SpikeTrains\"\n",
    "\n",
    "trainingData = expermentDataloader(indexFile5mm,PathTo5mmSpikeTrains)\n",
    "\n",
    "print(f\"Is NOT all zeros: {np.any(np.array(trainingData[0][0]))}\")\n",
    "print(len(trainingData[0][0]))\n",
    "print(f\"Number of spikes: {sum(trainingData[0][0])}\")\n",
    "trainingData[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is NOT all zeros: True\n",
      "2\n",
      "Number of spikes: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Is NOT all zeros: {np.any(np.array(trainingData[1][0]))}\")\n",
    "print(len(trainingData[1][0]))\n",
    "print(f\"Number of spikes: {sum(trainingData[1][0])}\")\n",
    "trainingData[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is NOT all zeros: True\n",
      "2\n",
      "Number of spikes: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Is NOT all zeros: {np.any(np.array(trainingData[len(trainingData)-1][0]))}\")\n",
    "print(len(trainingData[len(trainingData)-1][0]))\n",
    "print(f\"Number of spikes: {sum(trainingData[len(trainingData)-1][0])}\")\n",
    "trainingData[len(trainingData)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_spike_train = len(trainingData[len(trainingData)-1][0])\n",
    "steps_per_spike_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### setup dataloader for pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset=trainingData, batch_size=3)\n",
    "i = iter(train_loader)\n",
    "example = next(i)\n",
    "type(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor([0, 1, 0])\n",
      "tensor(True)\n",
      "tensor([2, 1, 0])\n",
      "tensor(True)\n",
      "tensor([2, 0, 0])\n",
      "tensor(True)\n",
      "tensor([0, 0, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(example[0][0].any())\n",
    "print(example[1])\n",
    "example = next(i)\n",
    "print(example[0][0].any())\n",
    "print(example[1])\n",
    "example = next(i)\n",
    "print(example[0][0].any())\n",
    "print(example[1])\n",
    "example = next(i)\n",
    "print(example[0][0].any())\n",
    "print(example[1])\n",
    "example[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shoul see the classes 1,0,2 and all true for any (i.e. they all have at least one event)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Building network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `slayer.block` is a combination of `synapse`, `dendrite`, `neuron` and `axon` components. This allows for easier devlepment of SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        neuron_params = {\n",
    "                'threshold'     : 0.1,\n",
    "                'current_decay' : 1,\n",
    "                'voltage_decay' : 0.1,\n",
    "                'requires_grad' : True,     \n",
    "            }\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                slayer.block.cuba.Dense(neuron_params, 2, 256),\n",
    "                slayer.block.cuba.Dense(neuron_params, 256, 2),\n",
    "            ])\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        return spike\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_folder = 'Trained'\n",
    "os.makedirs(trained_folder, exist_ok=True)\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda') \n",
    "\n",
    "net = Network().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pick between the following \n",
    "* `SpikeTime`: precise spike time based loss when target spike train is known.\n",
    "* `SpikeRate`: spike rate based loss when desired rate of the output neuron is known.\n",
    "* `SpikeMax`: negative log likelihood losses for classification without any rate tuning.\n",
    "Sense we dont know the output spike train (as of now) and we dont have a desired spike rate we use `SpikeMax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = slayer.loss.SpikeMax().to(device)\n",
    "stats = slayer.utils.LearningStats()\n",
    "assistant = slayer.utils.Assistant(net, error, optimizer, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0/5000] Train loss =         inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conda/feedstock_root/build_artifacts/pytorch-recipe_1673730874951/work/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m i, (\u001b[39minput\u001b[39m, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader): \u001b[39m# training loop\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m         output \u001b[39m=\u001b[39m assistant\u001b[39m.\u001b[39;49mtrain(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m      6\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m[Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m3d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m{\u001b[39;00mstats\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m stats\u001b[39m.\u001b[39mtraining\u001b[39m.\u001b[39mbest_loss:\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/site-packages/lava/lib/dl/slayer/utils/assistant.py:129\u001b[0m, in \u001b[0;36mAssistant.train\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstats \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstats\u001b[39m.\u001b[39mtraining\u001b[39m.\u001b[39mnum_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstats\u001b[39m.\u001b[39mtraining\u001b[39m.\u001b[39mloss_sum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitem() \\\n\u001b[1;32m    130\u001b[0m         \u001b[39m*\u001b[39m output\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:   \u001b[39m# classification\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstats\u001b[39m.\u001b[39mtraining\u001b[39m.\u001b[39mcorrect_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(\n\u001b[1;32m    133\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(output) \u001b[39m==\u001b[39m target\n\u001b[1;32m    134\u001b[0m         )\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (input, target) in enumerate(train_loader): # training loop\n",
    "        output = assistant.train(input, target)\n",
    "        print(f'\\r[Epoch {epoch:3d}/{epochs}] {stats}', end='')\n",
    "    \n",
    "    if stats.training.best_loss:\n",
    "        torch.save(net.state_dict(), trained_folder + '/network.pt')\n",
    "    stats.update()\n",
    "    stats.save(trained_folder + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
