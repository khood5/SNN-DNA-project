{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "from torch import nn\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from dnaDataloader import expermentDataloader\n",
    "from dnaDataloader import addData\n",
    "from scipy import stats as st\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "mp.set_start_method('spawn')\n",
    "batch_size = 25\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval with small traing sets\n",
    "The goal here is to eval the model while training on small sets of data. That is how well can MLP learn from 100-200 examples of single-molecule experiments.\n",
    "A verity of samples have been provided anf preprocessed (converted from excel to csv and had empty frames added) to the folder <pre>/home/khood/GitHub/SNN-DNA-project/Prepocessing/sorted</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = [d[0] for d in os.walk(\"/home/khood/GitHub/SNN-DNA-project/Prepocessing/sorted\")][1:] # remove first one is it is \"/home/khood/GitHub/SNN-DNA-project/Prepocessing/sorted\"\n",
    "len(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "featIn = 0\n",
    "for d in folders:\n",
    "    data = expermentDataloader(\n",
    "        f\"{d}/index.csv\",\n",
    "        f\"{d}\", \n",
    "    )\n",
    "    rawData = [d for d in data]\n",
    "    featIn = len(rawData[0][0])\n",
    "    trainValidData = []\n",
    "    testData = []\n",
    "    addData(testData, trainValidData, rawData, rhsSize=300)\n",
    "\n",
    "\n",
    "    np.random.shuffle(trainValidData)\n",
    "    trainData = []\n",
    "    validData = []\n",
    "    addData(trainData, validData, trainValidData, rhsSize=int(len(trainValidData)*(1/3)))\n",
    "\n",
    "    datasets.append({\"name\": f\"{os.path.basename(d)}\", \n",
    "                     \"train\":DataLoader(trainData, batch_size=batch_size, shuffle=True) , \n",
    "                     \"valid\":DataLoader(validData, batch_size=batch_size, shuffle=True) , \n",
    "                     \"test\":DataLoader(testData, batch_size=len(testData), shuffle=True) ,\n",
    "                     \"model\": {}}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets:\n",
      "{'name': '1800_nM_AR_out', 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1010cd7c70>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7f100aff6c50>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f100b119db0>, 'model': {}}\n",
      "{'name': '800_nM_AR_out', 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1010d08190>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7f100ae68580>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f100aeb6890>, 'model': {}}\n",
      "{'name': '1200_nM_AR_out', 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f100ae73ca0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7f100aeb41c0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f100aeeffa0>, 'model': {}}\n",
      "{'name': '400_nM_AR_out', 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f100aeb7e20>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7f100aeec220>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f100af2beb0>, 'model': {}}\n",
      "{'name': '50_nM_AR_out', 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f100aeefeb0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7f100af280a0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f100451feb0>, 'model': {}}\n",
      "{'name': '100_nM_AR_out', 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f100af2bfa0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7f100451c430>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f100455bc70>, 'model': {}}\n",
      "6\n",
      "featIn: 12000\n"
     ]
    }
   ],
   "source": [
    "print(f\"datasets:\")\n",
    "for d in datasets:\n",
    "    print(d)\n",
    "print(f\"{len(datasets)}\")\n",
    "print(f\"featIn: {featIn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 1800_nM_AR_out --\n",
      "train: 8\n",
      "valid: 4\n",
      "test : 1\n",
      "-- 800_nM_AR_out --\n",
      "train: 8\n",
      "valid: 4\n",
      "test : 1\n",
      "-- 1200_nM_AR_out --\n",
      "train: 8\n",
      "valid: 4\n",
      "test : 1\n",
      "-- 400_nM_AR_out --\n",
      "train: 8\n",
      "valid: 4\n",
      "test : 1\n",
      "-- 50_nM_AR_out --\n",
      "train: 8\n",
      "valid: 4\n",
      "test : 1\n",
      "-- 100_nM_AR_out --\n",
      "train: 8\n",
      "valid: 4\n",
      "test : 1\n"
     ]
    }
   ],
   "source": [
    "for d in datasets:\n",
    "    print(f\"-- {d['name']} --\")\n",
    "    print(f\"train: {len(d['train'])}\")\n",
    "    print(f\"valid: {len(d['valid'])}\")\n",
    "    print(f\"test : {len(d['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = mp.Manager()\n",
    "return_dict = manager.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_def import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Process name='Process-2' parent=1242456 initial>,\n",
       " <Process name='Process-3' parent=1242456 initial>,\n",
       " <Process name='Process-4' parent=1242456 initial>,\n",
       " <Process name='Process-5' parent=1242456 initial>,\n",
       " <Process name='Process-6' parent=1242456 initial>,\n",
       " <Process name='Process-7' parent=1242456 initial>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processes = []\n",
    "devices = [torch.device(\"cuda:0\"),torch.device(\"cuda:1\"),torch.device(\"cuda:2\"),torch.device(\"cuda:3\")]\n",
    "epochs = 10000\n",
    "error_margin = 20\n",
    "for d in datasets:\n",
    "    processes.append(mp.Process(target=train, args=(d[\"train\"], d[\"test\"], d[\"name\"], featIn, return_dict, epochs, error_margin, devices[0])))\n",
    "    devices.append(devices.pop(0))\n",
    "    \n",
    "processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 1800_nM_AR_out...\n",
      "training 800_nM_AR_out...\n",
      "training 1200_nM_AR_out...\n",
      "training 400_nM_AR_out...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m         processes[i]\u001b[39m.\u001b[39mstart()\n\u001b[1;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m run:\n\u001b[0;32m----> 9\u001b[0m         processes[i]\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m     10\u001b[0m         processes[i]\u001b[39m.\u001b[39mterminate()\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(return_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_pid \u001b[39m==\u001b[39m os\u001b[39m.\u001b[39mgetpid(), \u001b[39m'\u001b[39m\u001b[39mcan only join a child process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mcan only join a started process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_popen\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[39m.\u001b[39mdiscard(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/multiprocessing/popen_fork.py:43\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[39m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(os\u001b[39m.\u001b[39;49mWNOHANG \u001b[39mif\u001b[39;49;00m timeout \u001b[39m==\u001b[39;49m \u001b[39m0.0\u001b[39;49m \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     44\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mwaitpid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpid, flag)\n\u001b[1;32m     28\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[39m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[39m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processesList = list(range(len(processes)))\n",
    "\n",
    "while processesList:\n",
    "    run = processesList[:4]\n",
    "    processesList = processesList[4:]\n",
    "    for i in run:\n",
    "        processes[i].start()\n",
    "    for i in run:\n",
    "        processes[i].join()\n",
    "        processes[i].terminate()\n",
    "print(return_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
