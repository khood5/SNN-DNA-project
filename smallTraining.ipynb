{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "from torch import nn\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from dnaDataloader import expermentDataloader\n",
    "from dnaDataloader import addData\n",
    "from scipy import stats as st\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "mp.set_start_method('spawn')\n",
    "batch_size = 25\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval with small traing sets\n",
    "The goal here is to eval the model while training on small sets of data. That is how well can MLP learn from 100-200 examples of single-molecule experiments.\n",
    "A verity of samples have been provided anf preprocessed (converted from excel to csv and had empty frames added) to the folder <pre>/home/khood/GitHub/SNN-DNA-project/Prepocessing/sorted</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = [d[0] for d in os.walk(\"/home/khood/GitHub/SNN-DNA-project/Prepocessing/sorted\")][1:] # remove first one is it is \"/home/khood/GitHub/SNN-DNA-project/Prepocessing/sorted\"\n",
    "len(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "featIn = 0\n",
    "for d in folders:\n",
    "    data = expermentDataloader(\n",
    "        f\"{d}/index.csv\",\n",
    "        f\"{d}\", \n",
    "    )\n",
    "    rawData = [d for d in data]\n",
    "    featIn = len(rawData[0][0])\n",
    "    trainValidData = []\n",
    "    testData = []\n",
    "    addData(testData, trainValidData, rawData, rhsSize=300)\n",
    "\n",
    "\n",
    "    np.random.shuffle(trainValidData)\n",
    "    trainData = []\n",
    "    validData = []\n",
    "    addData(trainData, validData, trainValidData, rhsSize=int(len(trainValidData)*(1/3)))\n",
    "\n",
    "    datasets.append({\"name\": f\"{os.path.basename(d)}\", \n",
    "                     \"train\":DataLoader(trainData, batch_size=batch_size, shuffle=True) , \n",
    "                     \"valid\":DataLoader(validData, batch_size=batch_size, shuffle=True) , \n",
    "                     \"test\":DataLoader(testData, batch_size=len(testData), shuffle=True) ,\n",
    "                     \"error_margin\": None\n",
    "                     \"model\": {}}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets:\n",
      "{'name': '1800_nM_AR_out', 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1dffd53490>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7f1dffc4b850>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1dffadde70>, 'model': {}}\n",
      "{'name': '800_nM_AR_out', 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1dffd53be0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7f1dffa8ff70>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1dffaf6560>, 'model': {}}\n",
      "{'name': '1200_nM_AR_out', 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1e0590bca0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7f1dffaf4040>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1dffb2feb0>, 'model': {}}\n",
      "{'name': '400_nM_AR_out', 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1dffaf7f70>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7f1dffb2c430>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1df912ffa0>, 'model': {}}\n",
      "{'name': '50_nM_AR_out', 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1dffb2ffa0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7f1df912c340>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1df915ffa0>, 'model': {}}\n",
      "{'name': '100_nM_AR_out', 'train': <torch.utils.data.dataloader.DataLoader object at 0x7f1df912feb0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7f1df915c160>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f1df9193d30>, 'model': {}}\n",
      "6\n",
      "featIn: 12000\n"
     ]
    }
   ],
   "source": [
    "print(f\"datasets:\")\n",
    "for d in datasets:\n",
    "    print(d)\n",
    "print(f\"{len(datasets)}\")\n",
    "print(f\"featIn: {featIn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 1800_nM_AR_out --\n",
      "train: 8\n",
      "valid: 4\n",
      "test : 1\n",
      "-- 800_nM_AR_out --\n",
      "train: 8\n",
      "valid: 4\n",
      "test : 1\n",
      "-- 1200_nM_AR_out --\n",
      "train: 8\n",
      "valid: 4\n",
      "test : 1\n",
      "-- 400_nM_AR_out --\n",
      "train: 8\n",
      "valid: 4\n",
      "test : 1\n",
      "-- 50_nM_AR_out --\n",
      "train: 8\n",
      "valid: 4\n",
      "test : 1\n",
      "-- 100_nM_AR_out --\n",
      "train: 8\n",
      "valid: 4\n",
      "test : 1\n"
     ]
    }
   ],
   "source": [
    "for d in datasets:\n",
    "    print(f\"-- {d['name']} --\")\n",
    "    print(f\"train: {len(d['train'])}\")\n",
    "    print(f\"valid: {len(d['valid'])}\")\n",
    "    print(f\"test : {len(d['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = mp.Manager()\n",
    "return_dict = manager.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_def import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Process name='Process-2' parent=1041680 initial>,\n",
       " <Process name='Process-3' parent=1041680 initial>,\n",
       " <Process name='Process-4' parent=1041680 initial>,\n",
       " <Process name='Process-5' parent=1041680 initial>,\n",
       " <Process name='Process-6' parent=1041680 initial>,\n",
       " <Process name='Process-7' parent=1041680 initial>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processes = []\n",
    "devices = [torch.device(\"cuda:0\"),torch.device(\"cuda:1\"),torch.device(\"cuda:2\"),torch.device(\"cuda:3\")]\n",
    "for d in datasets:\n",
    "    processes.append(mp.Process(target=train, args=(d[\"train\"], d[\"test\"], d[\"name\"], featIn, return_dict, 60, devices[0])))\n",
    "    devices.append(devices.pop(0))\n",
    "    \n",
    "processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 1800_nM_AR_out...\n",
      "training 800_nM_AR_out...\n",
      "training 1200_nM_AR_out...\n",
      "training 400_nM_AR_out...\n",
      "training 50_nM_AR_out...\n",
      "training 100_nM_AR_out...\n",
      "{'1800_nM_AR_out': {'path': './Models/smallTrain/1800_nM_AR_out_18.05.2023_16-51-02-722778.pt', 'acc': 0.37244897959183676}, '800_nM_AR_out': {'path': './Models/smallTrain/800_nM_AR_out_18.05.2023_16-51-03-577436.pt', 'acc': 0.10606060606060606}, '1200_nM_AR_out': {'path': './Models/smallTrain/1200_nM_AR_out_18.05.2023_16-51-04-609774.pt', 'acc': 0.3765182186234818}, '400_nM_AR_out': {'path': './Models/smallTrain/400_nM_AR_out_18.05.2023_16-51-05-649048.pt', 'acc': 0.2655367231638418}, '50_nM_AR_out': {'path': './Models/smallTrain/50_nM_AR_out_18.05.2023_16-51-10-168149.pt', 'acc': 0.75}, '100_nM_AR_out': {'path': './Models/smallTrain/100_nM_AR_out_18.05.2023_16-51-11-036066.pt', 'acc': 0.3417085427135678}}\n"
     ]
    }
   ],
   "source": [
    "processesList = list(range(len(processes)))\n",
    "\n",
    "while processesList:\n",
    "    run = processesList[:4]\n",
    "    processesList = processesList[4:]\n",
    "    for i in run:\n",
    "        processes[i].start()\n",
    "    for i in run:\n",
    "        processes[i].join()\n",
    "        processes[i].terminate()\n",
    "print(return_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
