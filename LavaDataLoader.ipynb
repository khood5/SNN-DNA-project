{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with data from <code>Prepocessing/data/spikeTrains</code> should have folder for each experment setup (in terms of disctance of magnent in mm). Each folder should have a list of spike trains as csv files. Spike trains are recorded as events per 0.05 secs. The index file has the list of each spike train path and the class they are in. Classes are configureable via the preprocessing but should be somthing like this \n",
    "\n",
    "<table style=\"border:none;padding: 10px;margin: auto;\">\n",
    "    <tr style=\"border:none;padding: 10px;margin: auto;\">\n",
    "        <td style=\"border:none;padding: 10px;margin: auto;\">\n",
    "        9/5mm\n",
    "        <table>\n",
    "            <th>Class</th><th>Number of reactions</th>\n",
    "            <tr> <td>High</td> <td>>300</td> </tr>\n",
    "            <tr> <td>Medium</td> <td>300-230</td> </tr>\n",
    "            <tr> <td>Low</td> <td>&lt;230</td> </tr>\n",
    "        </table>\n",
    "        </td>\n",
    "        <td style=\"border:none;padding: 10px;margin: auto;\">\n",
    "        3mm\n",
    "        <table>\n",
    "            <th>Class</th><th>Number of reactions</th>\n",
    "            <tr> <td>High</td> <td>>150</td> </tr>\n",
    "            <tr> <td>Medium</td> <td>100-150</td> </tr>\n",
    "            <tr> <td>Low</td> <td>&lt;100</td> </tr>\n",
    "        </table>\n",
    "        </td>\n",
    "        <td style=\"border:none;padding: 10px;margin: auto;\">\n",
    "        2/1.5mm\n",
    "        <table>\n",
    "            <th>Class</th><th>Number of reactions</th>\n",
    "            <tr> <td>High</td> <td>>50</td> </tr>\n",
    "            <tr> <td>Medium</td> <td>30-50</td> </tr>\n",
    "            <tr> <td>Low</td> <td>&lt;30</td> </tr>\n",
    "        </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "We are using Intel's lava-nc framework https://lava-nc.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "# progress_bar for when stuff takes a while to load\n",
    "def progress_bar(current, total, bar_length=20):\n",
    "    fraction = current / total\n",
    "\n",
    "    arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "    padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "    ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "    print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our data loader. Should get an index file for an experment type (9mm,5mm etc). The index file should contain a path to each indiviual training experiment and the class it belongs to high, medium, or low.\n",
    "Note that the class is represented as a int, the map is shown below\n",
    "\n",
    "|class         | numerical value|\n",
    "|--------------|----------------|\n",
    "|<i>high</i>   |               2|\n",
    "|<i>medium</i> |               1|\n",
    "|<i>low</i>    |               0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example index file (Prepocessing/data/spikeTrains/1.5-SpikeTrains/index.csv)\n",
    "<pre>\n",
    "spikeTrain_1.csv,0\n",
    "spikeTrain_2.csv,0\n",
    "spikeTrain_3.csv,1\n",
    "spikeTrain_4.csv,0\n",
    "spikeTrain_5.csv,2\n",
    "spikeTrain_6.csv,1\n",
    "</pre>\n",
    "\n",
    "\n",
    "Example spik train file (Prepocessing/data/spikeTrains/1.5-SpikeTrains/spikeTrain_1.csv)\n",
    "<pre>\n",
    "0\n",
    "0\n",
    "0\n",
    ".\n",
    ".\n",
    ".\n",
    "0\n",
    "0\n",
    "1\n",
    "0\n",
    ".\n",
    ".\n",
    ".\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class expermentDataloader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_file: str, \n",
    "        data_path: str,\n",
    "    ):\n",
    "        self.root_dir = data_path\n",
    "        self.expermentSikeTrainsIndex = pd.read_csv(index_file) # self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.spikeTrains = [\n",
    "            f\"{os.path.join(self.expermentSikeTrainsIndex.iloc[i, 0])}\" for i in range(len(self.expermentSikeTrainsIndex)) \n",
    "        ]\n",
    "        self.expermentClasses = self.expermentSikeTrainsIndex.iloc[:, 1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        CSVlines = pd.read_csv(os.path.join(self.root_dir,self.spikeTrains[index])).to_numpy()\n",
    "        eventClass = self.expermentClasses[index]\n",
    "        return np.array(list(CSVlines.flatten())), int(eventClass)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.expermentSikeTrainsIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is NOT all zeros: True\n",
      "2999\n",
      "Number of spikes: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0]), 0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexFile5mm = \"./Prepocessing/data/spikeTrains/5-SpikeTrains/index.csv\"\n",
    "PathTo5mmSpikeTrains = \"./Prepocessing/data/spikeTrains/5-SpikeTrains\"\n",
    "\n",
    "trainingData = expermentDataloader(indexFile5mm,PathTo5mmSpikeTrains)\n",
    "\n",
    "print(f\"Is NOT all zeros: {np.any(trainingData[0][0])}\")\n",
    "print(len(trainingData[0][0]))\n",
    "print(f\"Number of spikes: {sum(trainingData[0][0])}\")\n",
    "trainingData[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is NOT all zeros: True\n",
      "2999\n",
      "Number of spikes: 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0]), 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Is NOT all zeros: {np.any(trainingData[1][0])}\")\n",
    "print(len(trainingData[1][0]))\n",
    "print(f\"Number of spikes: {sum(trainingData[1][0])}\")\n",
    "trainingData[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is NOT all zeros: True\n",
      "2999\n",
      "Number of spikes: 51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0]), 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Is NOT all zeros: {np.any(trainingData[len(trainingData)-1][0])}\")\n",
    "print(len(trainingData[len(trainingData)-1][0]))\n",
    "print(f\"Number of spikes: {sum(trainingData[len(trainingData)-1][0])}\")\n",
    "trainingData[len(trainingData)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_spike_train = len(trainingData[len(trainingData)-1][0])\n",
    "steps_per_spike_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Training STDP Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like lava-dl is what we want. This is a library that allows you to use lava-nc as a deep NN. There is a tutorial <a href=https://github.com/lava-nc/lava-dl/blob/a4fd4fc2ecd0da89e4aa0e9516533e85d4fbbcb0/tutorials/lava/lib/dl/slayer/nmnist/train.ipynb>here</a> that seems to be close to what we want.\n",
    "\n",
    "There are two ways to use the lib\n",
    "1. lava.lib.dl.slayer for natively training Deep Event-Based Networks.\n",
    "2. lava.lib.dl.bootstrap for training rate coded SNNs.\n",
    "\n",
    "We are going to try the first way, natively training with slayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "import zipfile\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class lava.lib.dl.slayer.block.cuba.Dense(args, kwargs)\n",
    "\n",
    "Bases: AbstractCuba, AbstractDense\n",
    "\n",
    "CUBA LIF dense block class. The block is 8 bit quantization ready.\n",
    "\n",
    "Parameters\n",
    "\n",
    "* neuron_params (dict, optional) – a dictionary of CUBA LIF neuron parameter. Defaults to None.\n",
    "\n",
    "* in_neurons (int) – number of input neurons.\n",
    "\n",
    "* out_neurons (int) – number of output neurons.\n",
    "\n",
    "* weight_scale (int, optional) – weight initialization scaling. Defaults to 1.\n",
    "\n",
    "* weight_norm (bool, optional) – flag to enable weight normalization. Defaults to False.\n",
    "\n",
    "* pre_hook_fx (optional) – a function pointer or lambda that is applied to synaptic weights before synaptic operation. None means no transformation. Defaults to None.\n",
    "\n",
    "* delay (bool, optional) – flag to enable axonal delay. Defaults to False.\n",
    "\n",
    "* delay_shift (bool, optional) – flag to simulate spike propagation delay from one layer to next. Defaults to True.\n",
    "\n",
    "* mask (bool array, optional) – boolean synapse mask that only enables relevant synapses. None means no masking is applied. Defaults to None.\n",
    "\n",
    "* count_log (bool, optional) – flag to return event count log. If True, an additional value of average event rate is returned. Defaults to False.\n",
    "\n",
    "training: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        neuron_params = {\n",
    "                'threshold'     : 1.25,\n",
    "                'current_decay' : 0.25,\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 0.03,\n",
    "                'scale_grad'    : 3,\n",
    "                'requires_grad' : True,     \n",
    "            }\n",
    "        neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                slayer.block.cuba.Dense(neuron_params_drop, 1, 512, weight_norm=True, delay=True),\n",
    "                slayer.block.cuba.Dense(neuron_params_drop, 512, 512, weight_norm=True, delay=True),\n",
    "                slayer.block.cuba.Dense(neuron_params, 512, 10, weight_norm=True),\n",
    "            ])\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        return spike\n",
    "    \n",
    "    def grad_flow(self, path):\n",
    "        # helps monitor the gradient flow\n",
    "        grad = [b.synapse.grad_norm for b in self.blocks if hasattr(b, 'synapse')]\n",
    "\n",
    "        plt.figure()\n",
    "        plt.semilogy(grad)\n",
    "        plt.savefig(path + 'gradFlow.png')\n",
    "        plt.close()\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_folder = 'Trained'\n",
    "os.makedirs(trained_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "net = Network().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=trainingData, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple loss functions we can use \n",
    "* `SpikeTime`: precise spike time based loss when target spike train is known.\n",
    "* `SpikeRate`: spike rate based loss when desired rate of the output neuron is known.\n",
    "* `SpikeMax`: negative log likelihood losses for classification without any rate tuning.\n",
    "\n",
    "We will use SpikeRate for our simple classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = slayer.loss.SpikeRate(true_rate=0.2, false_rate=0.03, reduction='sum').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the untils for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = slayer.utils.LearningStats()\n",
    "assistant = slayer.utils.Assistant(net, error, optimizer, stats, classifier=slayer.classifier.Rate.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay lets try the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [32, 2999]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m i, (\u001b[39minput\u001b[39m, label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader): \u001b[39m# training loop\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m         output \u001b[39m=\u001b[39m assistant\u001b[39m.\u001b[39;49mtrain(\u001b[39minput\u001b[39;49m, label)\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m[Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m2d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m{\u001b[39;00mstats\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[39m# for i, (input, label) in enumerate(test_loader): # training loop\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39m#     output = assistant.test(input, label)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39m# print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/site-packages/lava/lib/dl/slayer/utils/assistant.py:121\u001b[0m, in \u001b[0;36mAssistant.train\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlam \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         output, net_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(\u001b[39minput\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[33], line 23\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, spike)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, spike):\n\u001b[1;32m     22\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m---> 23\u001b[0m         spike \u001b[39m=\u001b[39m block(spike)\n\u001b[1;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m spike\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/site-packages/lava/lib/dl/slayer/block/base.py:515\u001b[0m, in \u001b[0;36mAbstractDense.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msynapse\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask\n\u001b[0;32m--> 515\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msynapse(x)\n\u001b[1;32m    516\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneuron(z)\n\u001b[1;32m    517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelay_shift \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/site-packages/lava/lib/dl/slayer/synapse/layer.py:172\u001b[0m, in \u001b[0;36mDense.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv3d(  \u001b[39m# bias does not need pre_hook_fx. Its disabled\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mreshape(old_shape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, old_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]),\n\u001b[1;32m    168\u001b[0m         weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    169\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[1;32m    170\u001b[0m     )\u001b[39m.\u001b[39mreshape(old_shape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, old_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    171\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv3d(\n\u001b[1;32m    173\u001b[0m         \u001b[39minput\u001b[39;49m, weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    174\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups,\n\u001b[1;32m    175\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [32, 2999]"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (input, label) in enumerate(train_loader): # training loop\n",
    "        output = assistant.train(input, label)\n",
    "    print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n",
    "        \n",
    "    # for i, (input, label) in enumerate(test_loader): # training loop\n",
    "    #     output = assistant.test(input, label)\n",
    "    # print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n",
    "        \n",
    "    if epoch%20 == 19: # cleanup display\n",
    "        print('\\r', ' '*len(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}'))\n",
    "        stats_str = str(stats).replace(\"| \", \"\\n\")\n",
    "        print(f'[Epoch {epoch:2d}/{epochs}]\\n{stats_str}')\n",
    "    \n",
    "    if stats.testing.best_accuracy:\n",
    "        torch.save(net.state_dict(), trained_folder + '/network.pt')\n",
    "    stats.update()\n",
    "    stats.save(trained_folder + '/')\n",
    "    net.grad_flow(trained_folder + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Truing LAVA dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class expermentDataloader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_file: str, \n",
    "        data_path: str,\n",
    "    ):\n",
    "        self.root_dir = data_path\n",
    "        self.expermentSikeTrainsIndex = pd.read_csv(index_file) # self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.spikeTrains = [\n",
    "            f\"{os.path.join(self.expermentSikeTrainsIndex.iloc[i, 0])}\" for i in range(len(self.expermentSikeTrainsIndex)) \n",
    "        ]\n",
    "        self.expermentClasses = self.expermentSikeTrainsIndex.iloc[:, 1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        CSVlines = pd.read_csv(os.path.join(self.root_dir,self.spikeTrains[index])).to_numpy()\n",
    "        eventClass = self.expermentClasses[index]\n",
    "        return np.array(list(CSVlines.flatten())), int(eventClass)\n",
    "\n",
    "    def __index__(self, index):\n",
    "        return self.__getitem__(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.expermentSikeTrainsIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is NOT all zeros: True\n",
      "2999\n",
      "Number of spikes: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0]), 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexFile5mm = \"./Prepocessing/data/spikeTrains/5-SpikeTrains/index.csv\"\n",
    "PathTo5mmSpikeTrains = \"./Prepocessing/data/spikeTrains/5-SpikeTrains\"\n",
    "\n",
    "trainingData = expermentDataloader(indexFile5mm,PathTo5mmSpikeTrains)\n",
    "\n",
    "print(f\"Is NOT all zeros: {np.any(trainingData[0][0])}\")\n",
    "print(len(trainingData[0][0]))\n",
    "print(f\"Number of spikes: {sum(trainingData[0][0])}\")\n",
    "trainingData[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import typing as ty\n",
    "\n",
    "# Import premade Processes\n",
    "from lava.proc.dense.process import Dense\n",
    "from lava.proc.lif.process import LIF\n",
    "\n",
    "# Import Processes and Process level primitives\n",
    "from lava.magma.core.process.process import AbstractProcess\n",
    "from lava.magma.core.process.variable import Var\n",
    "from lava.magma.core.process.ports.ports import  OutPort, InPort\n",
    "\n",
    "# Import ProcessModels and ProcessModels level primitives\n",
    "from lava.magma.core.decorator import implements, requires\n",
    "from lava.magma.core.model.py.model import PyLoihiProcessModel\n",
    "from lava.magma.core.model.py.ports import PyOutPort, PyInPort\n",
    "from lava.magma.core.model.py.type import LavaPyType\n",
    "from lava.magma.core.resources import CPU\n",
    "\n",
    "# Import execution protocol and hardware resources\n",
    "from lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\n",
    "from lava.magma.core.run_configs import Loihi1SimCfg\n",
    "from lava.magma.core.run_conditions import RunSteps, RunContinuous\n",
    "\n",
    "# Import Monitor and graph stuff\n",
    "from lava.proc.monitor.process import Monitor\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with '''lava.proc.io.dataloader.SpikeDataloader''' see https://lava-nc.org/lava/lava.proc.io.html\n",
    "from lava.proc.io.dataloader import SpikeDataloader\n",
    "steps_per_spike_train = len(trainingData[len(trainingData)-1][0])\n",
    "input = SpikeDataloader(dataset=trainingData,interval=steps_per_spike_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'interval', 'offset']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.vars.member_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_spike_trains = 3\n",
    "output = Monitor()\n",
    "groundTruth = Monitor()\n",
    "output.probe(input.s_out, steps_per_spike_train*number_of_spike_trains)\n",
    "groundTruth.probe(input.ground_truth, steps_per_spike_train*number_of_spike_trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spike Train 0\n",
      "Encountered Fatal Exception: slice indices must be integers or None or have an __index__ method\n",
      "Traceback: Encountered Fatal Exception: slice indices must be integers or None or have an __index__ method\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/khood/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/runtime/runtime.py\", line 95, in target_fn\n",
      "    actor.start(*args, **kwargs)\n",
      "  File \"/home/khood/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/core/model/py/model.py\", line 83, in start\n",
      "    p.start()\n",
      "  File \"/home/khood/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/core/model/interfaces.py\", line 34, in start\n",
      "    csp_port.start()\n",
      "  File \"/home/khood/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/compiler/channels/pypychannel.py\", line 82, in start\n",
      "    self._array = [\n",
      "  File \"/home/khood/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/compiler/channels/pypychannel.py\", line 86, in <listcomp>\n",
      "    buffer=self._shm.buf[\n",
      "TypeError: slice indices must be integers or None or have an __index__ method\n",
      "Traceback: \n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/khood/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/runtime/runtime.py\", line 95, in target_fn\n",
      "    actor.start(*args, **kwargs)\n",
      "  File \"/home/khood/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/core/model/py/model.py\", line 83, in start\n",
      "    p.start()\n",
      "  File \"/home/khood/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/core/model/interfaces.py\", line 34, in start\n",
      "    csp_port.start()\n",
      "  File \"/home/khood/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/compiler/channels/pypychannel.py\", line 224, in start\n",
      "    self._array = [\n",
      "  File \"/home/khood/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/compiler/channels/pypychannel.py\", line 228, in <listcomp>\n",
      "    buffer=self._shm.buf[\n",
      "TypeError: slice indices must be integers or None or have an __index__ method\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m run_cfg \u001b[39m=\u001b[39m Loihi1SimCfg(select_tag\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloating_pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m run_condition \u001b[39m=\u001b[39m RunSteps(num_steps\u001b[39m=\u001b[39msteps_per_spike_train)\n\u001b[0;32m----> 7\u001b[0m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mrun(condition\u001b[39m=\u001b[39;49mrun_condition, run_cfg\u001b[39m=\u001b[39;49mrun_cfg)\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/core/process/process.py:349\u001b[0m, in \u001b[0;36mAbstractProcess.run\u001b[0;34m(self, condition, run_cfg, compile_config)\u001b[0m\n\u001b[1;32m    346\u001b[0m     executable\u001b[39m.\u001b[39massign_runtime_to_all_processes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_runtime)\n\u001b[1;32m    347\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_runtime\u001b[39m.\u001b[39minitialize()\n\u001b[0;32m--> 349\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_runtime\u001b[39m.\u001b[39;49mstart(condition)\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/runtime/runtime.py:311\u001b[0m, in \u001b[0;36mRuntime.start\u001b[0;34m(self, run_condition)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_initialized:\n\u001b[1;32m    309\u001b[0m     \u001b[39m# Start running\u001b[39;00m\n\u001b[1;32m    310\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_started \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(run_condition)\n\u001b[1;32m    312\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mRuntime not initialized yet.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/runtime/runtime.py:329\u001b[0m, in \u001b[0;36mRuntime._run\u001b[0;34m(self, run_condition)\u001b[0m\n\u001b[1;32m    327\u001b[0m         send_port\u001b[39m.\u001b[39msend(enum_to_np(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_steps))\n\u001b[1;32m    328\u001b[0m     \u001b[39mif\u001b[39;00m run_condition\u001b[39m.\u001b[39mblocking:\n\u001b[0;32m--> 329\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_resp_for_run()\n\u001b[1;32m    330\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(run_condition, RunContinuous):\n\u001b[1;32m    331\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_steps \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmaxsize\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/runtime/runtime.py:272\u001b[0m, in \u001b[0;36mRuntime._get_resp_for_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m rsps \u001b[39m=\u001b[39m []\n\u001b[1;32m    271\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     recv_port \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(\u001b[39m*\u001b[39;49mchannel_actions)\n\u001b[1;32m    273\u001b[0m     data \u001b[39m=\u001b[39m recv_port\u001b[39m.\u001b[39mrecv()\n\u001b[1;32m    274\u001b[0m     rsps\u001b[39m.\u001b[39mappend(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/site-packages/lava/magma/compiler/channels/pypychannel.py:318\u001b[0m, in \u001b[0;36mCspSelector.select\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_observer(args, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    317\u001b[0m         \u001b[39mreturn\u001b[39;00m action()\n\u001b[0;32m--> 318\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cv\u001b[39m.\u001b[39;49mwait()\n",
      "File \u001b[0;32m~/anaconda3/envs/dna/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_of_spike_trains = 1\n",
    "\n",
    "for i in range(number_of_spike_trains):\n",
    "    print(f\"Running Spike Train {i}\")\n",
    "    run_cfg = Loihi1SimCfg(select_tag=\"floating_pt\")\n",
    "    run_condition = RunSteps(num_steps=steps_per_spike_train)\n",
    "    input.run(condition=run_condition, run_cfg=run_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
